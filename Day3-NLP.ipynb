{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e82a3d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74025b4",
   "metadata": {},
   "source": [
    "## 1. Natural Language Processing - A Naive Example\n",
    "\n",
    "Before diving into real Twitter data, let’s start with a simple example.\n",
    "Here’s a small corpus consisting of three short documents:\n",
    "\n",
    "- Document 1: It is going to rain today.\n",
    "- Document 2: Today I am not going outside.\n",
    "- Document 3: I am going to watch the season premiere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8024a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Document1= \"It is going to rain today.\"\n",
    "Document2= \"Today I am not going outside.\"\n",
    "Document3= \"I am going to watch the season premiere.\"\n",
    "Doc = [Document1 ,\n",
    " Document2 , \n",
    " Document3]\n",
    "print(Doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0847ad76",
   "metadata": {},
   "source": [
    "\n",
    "From this example, we’ll learn how to convert raw text into numerical features — or what we might call columns of numbers. This process is often referred to as vectorization.\n",
    "\n",
    "Once we represent text as vectors, we unlock the ability to perform various types of analysis, including:\n",
    "- Summarization\n",
    "- Clustering\n",
    "- Topic modeling\n",
    "- Information retrieval (e.g., finding similar texts)\n",
    "- Predictive modeling\n",
    "\n",
    "The core idea in Natural Language Processing (NLP) is transforming unstructured text into structured numerical form. While there are many ways to do this, we’ll focus on one of the most widely used and interpretable methods: TF-IDF (Term Frequency–Inverse Document Frequency).\n",
    "\n",
    "TF-IDF is useful in many NLP applications. For example:\n",
    "- Search engines use it to rank the relevance of a document to a search query.\n",
    "- It’s also used in text classification, summarization, and topic modeling.\n",
    "\n",
    "After learning TF-IDF, we’ll apply it in a downstream task — topic modeling — to uncover hidden themes across the documents.\n",
    "\n",
    "While we won’t cover every vectorization technique or downstream task, this example will give you a strong foundation for understanding how an NLP pipeline works.\n",
    "\n",
    "\n",
    "### 1.1 Vectorization: Term Frequency(TF) — Inverse Document Frequency(IDF) Vectorization\n",
    "A corpus can be defined as a collection of documents. In our example, each sentence is a document, and they collectively form a corpus.  \n",
    "\n",
    "To vectorize text data, we use a TF-IDF method. \n",
    "- We first tokenize the text, and then assign an importance score for every term. \n",
    "- The importance score of a term is high when it occurs a lot in a given document and rarely in others. \n",
    "- In short, commonality within a document measured by TF is balanced by rarity between documents measured by IDF. The resulting TF-IDF score reflects the importance of a term for a document in the corpus.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a02001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer() #TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "analyze = vectorizer.build_analyzer()\n",
    "print(\"Document 1\",analyze(Document1))\n",
    "print(\"Document 2\",analyze(Document2))\n",
    "print(\"Document 3\",analyze(Document3))\n",
    "\n",
    "X = vectorizer.fit_transform(Doc)\n",
    "\n",
    "print(X)\n",
    "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3f3473",
   "metadata": {},
   "source": [
    "\n",
    "We tokenize and generate a vocab of the document. For each document, we can find the TF= (Number of repetitions of word in a document) / (# of words in a document). We can further find the IDF=Log[(Number of documents) / (Number of documents containing the word)]\n",
    "\n",
    "| words      | Doc1 | Doc2| Doc3|IDF Value|\n",
    "| ----------- | ----------- |----------- |----------- |----------- |\n",
    "| going      | 0.16     |0.16|0.12|0|\n",
    "| to   | 0.16       |0|0.12|0.41|\n",
    "|today|0.16|0.16|0|0.41|\n",
    "|i|0|0.16|0.12|0.41|\n",
    "|am|0|0.16|0.12|0.41|\n",
    "|it|0.16|0|0|1.09|\n",
    "|is |0.16|0|0|1.09|\n",
    "|rain|0.16|0|0|1.09|\n",
    "\n",
    "We then construct a document-term matrix using the TF-IDF scores:\n",
    "\n",
    "| Docs      | going |to|today|i|am|it|is|rain|\n",
    "| ------ |------ |------ |------ |------ |------ |------ |------ |------ |\n",
    "| Doc1      | 0  |0.07|0.07|0|0|0.17|0.17|0.17|0.17|\n",
    "| Doc2   | 0  |0|0.07|0.07|0.07|0|0|0|\n",
    "|Doc3|0|0.05|0|0.05|0.05|0|0|0|\n",
    "\n",
    "It is easy to see that 'it', 'is', and 'rain' are important for Doc 1 but not Doc 2 or Doc 3. Each row of the document-term matrix can be thought of as a numeric representation of the documents, which we often term vectors. These numeric representations help you to find similarities between documents. \n",
    " \n",
    "> You might have noticed that stop words such as “to” and “is” are included above. These are usually filtered out in real-world NLP tasks because they don’t carry much meaning.\n",
    "\n",
    "To perform vectorization in Python, we use the <code>TfidfVectorizer</code> from the <code>sklearn</code> package.\n",
    "\n",
    "The steps are:\n",
    "- Create the vectorizer.\n",
    "- Fit it on your corpus.\n",
    "- Transform your corpus into vectors.\n",
    "\n",
    "The function **TfidfVectorizer** takes two parameters. \n",
    "- max_df is used for removing terms that appear too frequently, also known as \"corpus-specific stop words\". For example:\n",
    "    - max_df = 0.50 means \"ignore terms that appear in more than 50% of the documents\".\n",
    "    - max_df = 25 means \"ignore terms that appear in more than 25 documents\".\n",
    "    - The default max_df is 1.0, which means \"ignore terms that appear in more than 100% of the documents\". Thus, the default setting does not ignore any terms.\n",
    "- min_df is used for removing terms that appear too infrequently. For example:\n",
    "    - min_df = 0.01 means \"ignore terms that appear in less than 1% of the documents\".\n",
    "    - min_df = 5 means \"ignore terms that appear in less than 5 documents\".\n",
    "    - The default min_df is 1, which means \"ignore terms that appear in less than 1 document\". Thus, the default setting does not ignore any terms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e450721",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=Doc\n",
    "#Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=0.1, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(docs)\n",
    "\n",
    "\n",
    "tfidf_df = pd.DataFrame.sparse.from_spmatrix(tfidf, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023bb20b",
   "metadata": {},
   "source": [
    "Now, after removing stop words, the resulting matrix looks like this (we'll call it M):\n",
    "\n",
    "|     | outside   | premiere | rain     | season   | today    | watch    |\n",
    "|-----|-----------|----------|----------|----------|----------|----------|\n",
    "| 0   | 0         | 0        | 0.795961 | 0        | 0.605349 | 0        |\n",
    "| 1   | 0.795961  | 0        | 0        | 0        | 0.605349 | 0        |\n",
    "| 2   | 0         | 0.57735  | 0        | 0.57735  | 0        | 0.57735  |\n",
    "\n",
    "\n",
    "It is notable that <code>tfidf</code> is a sparse matrix. If you'd like to view it as a full DataFrame, use:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d2879c",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2 Non-negative Matrix Factorization (NMF)\n",
    "\n",
    "TF-IDF vectors are great, but high-dimensional. When we have hundreds or thousands of terms, interpretation becomes difficult.\n",
    "\n",
    "To reduce this complexity and uncover latent themes, we use Non-negative Matrix Factorization (NMF), a powerful technique for **topic modeling**.\n",
    "\n",
    " If we think of the document-term matrix $M$ as a $m \\times n$ matrix with $m$ documents and $n$ terms, $M$ can be factorized as \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "M=W \\times H\n",
    "$$\n",
    "\n",
    "- M: Original document-term matrix (e.g., m docs × n terms)\n",
    "- W: Document-topic matrix (m docs × k topics)\n",
    "- H: Topic-term matrix (k topics × n terms)\n",
    "- k: Number of topics\n",
    "\n",
    "NMF finds W and H such that their product approximates M, and all values remain non-negative.\n",
    "\n",
    "This technique helps extract topics from text — where each topic is a combination of words, and each document can belong to multiple topics with different strengths.\n",
    " \n",
    " \n",
    "\n",
    "The function NMF takes two parameters. \n",
    "- n_components is the number of topics\n",
    "- random_state controls the random number generator used in the attribute combining process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7fd174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf_model = NMF(n_components=2, random_state=0)\n",
    "#nmf_model.fit(tfidf)\n",
    "W = nmf_model.fit_transform(tfidf)  # Document-topic matrix\n",
    "\n",
    "# Display topics\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "topic_names=[]\n",
    "# Assume nmf_model and feature_names are already defined\n",
    "topic_names = []\n",
    "\n",
    "# Loop through each topic\n",
    "for topic_index in range(len(nmf_model.components_)):\n",
    "    topic = nmf_model.components_[topic_index]\n",
    "    print(topic)\n",
    "    # Get the indices of the top 3 words (largest values in the topic)\n",
    "    sorted_indices = topic.argsort()  # sorts from smallest to largest\n",
    "\n",
    "    print(sorted_indices)\n",
    "    top_indices = sorted_indices[-3:]  # get the last 3 (top 3 words)\n",
    "    \n",
    "    # Reverse to make it largest to smallest\n",
    "    top_indices = top_indices[::-1]\n",
    "\n",
    "    # Get the actual word names for these indices\n",
    "    top_words = []\n",
    "    for i in top_indices:\n",
    "        top_words.append(feature_names[i])\n",
    "    \n",
    "    # Join the top words into a single string\n",
    "    top_words_string = \" \".join(top_words)\n",
    "\n",
    "    # Print and save\n",
    "    print(\"Topic #{}:\".format(topic_index))\n",
    "    print(top_words_string)\n",
    "    topic_names.append(top_words_string)\n",
    "topic_df = pd.DataFrame(W, columns=topic_names)\n",
    "topic_df\n",
    "\n",
    "topic_df = pd.DataFrame(nmf_model.components_ ,columns=feature_names)\n",
    "topic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee934b1",
   "metadata": {},
   "source": [
    "This is the W matrix (document-topic distribution):\n",
    "\n",
    "\n",
    "|     | today outside rain | watch season premiere |\n",
    "|-----|--------------------|------------------------|\n",
    "| 0   | 0.490981           | 0.000000               |\n",
    "| 1   | 0.490981           | 0.000000               |\n",
    "| 2   | 0.000000           | 0.840054               |\n",
    "\n",
    "And this is the H matrix (topic-word distribution):\n",
    "\n",
    "|     | outside   | premiere | rain     | season   | today    | watch    |\n",
    "|-----|-----------|----------|----------|----------|----------|----------|\n",
    "| 0   | 0.810582  | 0.000000 | 0.810582 | 0.000000 | 1.232936 | 0.000000 |\n",
    "| 1   | 0.000000  | 0.687278 | 0.000000 | 0.687278 | 0.000000 | 0.687278 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854d00b2",
   "metadata": {},
   "source": [
    "\n",
    "## 2 Analyzing Twitter Data\n",
    "\n",
    "Finally, we get to practice using the Twitter data! \n",
    "### 2.1 What Social Media Accounts to Search?\n",
    "\n",
    "To identify social media accounts related to AI tools, we perform a Google search using the keyword \"AI marketing tools\". Below are the Search Engine Results Pages (also known as “SERPs” or “SERP”).\n",
    "\n",
    "The first few results are sponsored links, and one organic result points us to [15 Best AI Marketing Tools in 2023-2024](https://improvado.io/blog/best-ai-marketing-tools). Among the recommended AI tools, we are particularly interested in [Grammarly](https://twitter.com/Grammarly). Let's collect tweets generated by Grammarly's official account and examine which tweets get more likes.\n",
    "\n",
    "\n",
    "> Grammarly is a cloud-based typing assistant. It reviews spelling, grammar, punctuation, clarity, engagement, and delivery mistakes in English texts, detects plagiarism, and suggests replacements for the identified errors. For a brief introduction to Grammarly, watch this [video](https://www.youtube.com/watch?v=zd64pGNLjVY).\n",
    "\n",
    "\n",
    "### 2.2. Data Collection\n",
    "Twitter has its API service. To simplify this data collection process, I built a little package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "101d77c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --upgrade --force-reinstall git+https://github.com/tantantan12/itom6219.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d82460e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>verified</th>\n",
       "      <th>username</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>public_metrics.followers_count</th>\n",
       "      <th>public_metrics.following_count</th>\n",
       "      <th>public_metrics.tweet_count</th>\n",
       "      <th>public_metrics.listed_count</th>\n",
       "      <th>public_metrics.like_count</th>\n",
       "      <th>public_metrics.media_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Grammarly</td>\n",
       "      <td>Good writing moves work forward. #StandWithUkr...</td>\n",
       "      <td>True</td>\n",
       "      <td>Grammarly</td>\n",
       "      <td>47191725</td>\n",
       "      <td>2009-06-14T22:23:52.000Z</td>\n",
       "      <td>227922</td>\n",
       "      <td>3455</td>\n",
       "      <td>41476</td>\n",
       "      <td>2849</td>\n",
       "      <td>21049</td>\n",
       "      <td>9977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name                                        description  verified  \\\n",
       "0  Grammarly  Good writing moves work forward. #StandWithUkr...      True   \n",
       "\n",
       "    username        id                created_at  \\\n",
       "0  Grammarly  47191725  2009-06-14T22:23:52.000Z   \n",
       "\n",
       "   public_metrics.followers_count  public_metrics.following_count  \\\n",
       "0                          227922                            3455   \n",
       "\n",
       "   public_metrics.tweet_count  public_metrics.listed_count  \\\n",
       "0                       41476                         2849   \n",
       "\n",
       "   public_metrics.like_count  public_metrics.media_count  \n",
       "0                      21049                        9977  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"BEARER_TOKEN\"] = \"AAAAAAAAAAAAAAAAAAAAAA7fGwEAAAAATek8qNEHmKiwy5NeLLGGLu%2FOllc%3DvMI6a81TOlLcj6fthUgm5xT66tHGcKYcklMRLcRZjxQBKpqWJp\"\n",
    "\n",
    "\n",
    "from itom6219 import user_info, user_tweets, user_tweets_all\n",
    "user=user_info([\"grammarly\"])\n",
    "user\n",
    "#tweets=user_tweets([\"grammarly\"], exclude_replies=True, exclude_retweets=True)\n",
    "\n",
    "#tweets_all=user_tweets_all([\"sunomusic\",\"TSwiftLyricsBot\"],max_total=1000, exclude_replies=True, exclude_retweets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f420f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching tweets for user Grammarly: 429\n"
     ]
    }
   ],
   "source": [
    "tweets=user_tweets([\"grammarly\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b690864e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'AI_tweets_all.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# We use pd.read_csv to read csv file\u001b[39;00m\n\u001b[32m      2\u001b[39m file_path = \u001b[33m'\u001b[39m\u001b[33mAI_tweets_all.csv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'AI_tweets_all.csv'"
     ]
    }
   ],
   "source": [
    "# We use pd.read_csv to read csv file\n",
    "file_path = 'AI_tweets_all.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471eb76f",
   "metadata": {},
   "source": [
    "#### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbcd898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "df['datetime']=pd.to_datetime(df['created_at'])\n",
    "\n",
    "df['log_view']=np.log1p(df['public_metrics.impression_count'])  \n",
    "df['log_view']=np.log1p(df['public_metrics.impression_count'])  \n",
    "\n",
    "# create a line plot with Plotly Express\n",
    "fig = px.line(df, x='datetime', y='log_view', title='Impression Over Time', template='plotly_white')\n",
    "\n",
    "# display the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c878df95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='log_view', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1911e535",
   "metadata": {},
   "source": [
    "## 3 Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564bde96",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=df['text']\n",
    "#Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(docs)\n",
    "\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169a0b0a",
   "metadata": {},
   "source": [
    "## 3.4 Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94cc8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply NMF\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf_model = NMF(n_components=7, random_state=0)\n",
    "nmf_model.fit(tfidf)\n",
    "W = nmf_model.fit_transform(tfidf)  # Document-topic matrix\n",
    "\n",
    "# Display topics\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "topic_names=[]\n",
    "for topic_index in range(len(nmf_model.components_)):\n",
    "    topic = nmf_model.components_[topic_index]\n",
    "    # Get the indices of the top 3 words (largest values in the topic)\n",
    "    sorted_indices = topic.argsort()  # sorts from smallest to largest\n",
    "    top_indices = sorted_indices[-4:]  # get the last 3 (top 3 words)\n",
    "    # Reverse to make it largest to smallest\n",
    "    top_indices = top_indices[::-1]\n",
    "    # Get the actual word names for these indices\n",
    "    top_words = []\n",
    "    for i in top_indices:\n",
    "        top_words.append(feature_names[i])\n",
    "    # Join the top words into a single string\n",
    "    top_words_string = \" \".join(top_words)\n",
    "    # Print and save\n",
    "    print(\"Topic #{}:\".format(topic_index))\n",
    "    print(top_words_string)\n",
    "    topic_names.append(top_words_string)\n",
    "\n",
    "topic_df = pd.DataFrame(W, columns=topic_names)\n",
    "topic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc250f82",
   "metadata": {},
   "source": [
    "\n",
    "## 4 Linear Regression\n",
    "\n",
    "Linear regression is one of the most commonly used techniques in data analysis. It helps us understand the relationship between one or more input variables (features) and an output variable (target). In the simplest case, it tries to draw a straight line that best fits the data.\n",
    "\n",
    "In our example, we want to understand:\n",
    "\n",
    "- How do the topics of Grammarly’s tweets influence the number of likes?\n",
    "- Which topics are more likely to lead to higher engagement (likes)?\n",
    "- Which topics seem to have less impact or even negative impact?\n",
    "\n",
    "Each tweet is represented as a set of topic weights (from NMF), and our target is the like count for that tweet.\n",
    "\n",
    "We’ll use the topic weights (<code>topic_df</code>) as features, and the like count (<code>df['public_metrics.like_count']</code>) as the target.\n",
    "\n",
    "\n",
    "The model assumes a relationship of the form:\n",
    "\n",
    "$$\n",
    "\\text{Like\\_Count} = \\beta_0 + \\beta_1 \\cdot \\text{Topic}_1 + \\beta_2 \\cdot \\text{Topic}_2 + \\dots + \\beta_k \\cdot \\text{Topic}_k\n",
    "$$\n",
    "\n",
    "- $\\beta_0$ is the intercept.  \n",
    "- $\\beta_1, \\beta_2, \\dots, \\beta_k$ are **coefficients** for each topic.  \n",
    "- A **positive coefficient** ($\\beta_i > 0$) means the topic is associated with **more likes**.  \n",
    "- A **negative coefficient** ($\\beta_i < 0$) means the topic is associated with **fewer likes**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b68b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pingouin as pg\n",
    "\n",
    "# Combine X and y into a single dataframe\n",
    "df_model = topic_df.copy()\n",
    "df_model['ratio'] = df['public_metrics.like_count'] / df['log_view']\n",
    "\n",
    "# Run linear regression\n",
    "result = pg.linear_regression(df_model.drop(columns='ratio'), df_model['ratio'])\n",
    "\n",
    "# Round coef and pval to 3 decimal places\n",
    "result[['names', 'coef', 'pval']] = result[['names', 'coef', 'pval']].round(3)\n",
    "\n",
    "# Display the rounded result\n",
    "result[['names', 'coef', 'pval']]\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
